{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxQbfnQlytttZo01SM6GKA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashy22asuedu/MAT_421/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Evaluating the performance of a basic SVD-based recommender system"
      ],
      "metadata": {
        "id": "d9KiY1khO3Yc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "vW9tRTN7Q9QN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommendation systems are important to businesses in the modern world. Advertising companies such as Google and Facebook would like to find a way to target their users with ads that they are most likely to click, e-commerce websites like Amazon would like to present you with products that you are most likely to buy, and media streaming services such as Spotify, YouTube, and Netflix want to recommend you media which you would most likely want to consume.\n",
        "\n",
        "What requirements would we have to take in to consideration when building a recommendation system? First of all, recommendations have to be *personalized* to be most effective. A recommendation system wouldn't be effective if everyone was just recommended the same thing, because different people have different preferences. Most recommendation systems the following two approaches to take this into consideration:\n",
        "1. Content-based filtering assumes that nothing is initially known about a user, but content (information) is known about the items which are going to be recommended, such as their name, description, date of creation, etc. Over time, as a user provides feedback on more and more items, a user profile can be built to predict which content they are more likely to like. While this is great for recommending items similar to what a user has already seen, this approach isn't great for generalizing across content types; for example, if we have built a model for a user that can predict what type of books they like, we can't use the same model as effectively to recommend them movies. Also, this approach requires that you have adequate information about every product in the first place, which may be nearly impossible to guarantee if you are on the scale of, say, YouTube and have millions of videos uploaded every day.\n",
        "2. Collaborative filtering instead uses information about people's previous activity, and assumes that people who have previously liked similar items in the past would like similar items in the future. An example of this second approach is what I am going to demonstrate in this project."
      ],
      "metadata": {
        "id": "vKSWilBwRBR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Related work"
      ],
      "metadata": {
        "id": "LCK--10Wdngj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In 2006, Netflix published an anonymized data set consisting of around 100m ratings from around 500k users for 18k users. Each data point was a quadruplet consisting of a user id, a movie id, a date when the rating was given, and an integer rating between 1 and 5. An submitted algorithm then had to predict missing ratings for given user id, movie id, and date of rating triplets. The performance of algorithms were rated on RMSE. Whichever team improved upon Netflix's algorithm by at least 10% first would win $1m.\n",
        "\n",
        "During the contest, Simon Funk introduced a novel algorithm [in a blog post](https://sifter.org/~simon/journal/20061211.html) for collaborative filtering based on SVD matrix factorization of the user-rating matrix and gradient descent.\n",
        "\n",
        "I will go into more detail and give an intuitive explanation about how the algorithm works in my project paper, but essentially the algorithm assumes assumes that there are a given number (n) of latent factors which relate users to movie ratings in a latent space; these latent factors are found using gradient descent by starting of with a random guess of what the best rank n approximation of the data may be, and using a loss function to measure how close the approximation is to the original data. The loss function is used for gradient descent. Since the original data is a sparsely filled matrix, the loss function only uses these data points to evaluate how accurate the rank n approximation is; the rest of the values we get at the end are the predictions under the latent factor hypothesis.\n",
        "\n",
        "One of the issues of newer matrix factorization based systems based on deep learning, which have been introduced at certain AI conferences, is that the results that their results have [not been reproducible](https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)#Deep-learning_MF). In fact [one paper](https://dl.acm.org/doi/10.1145/3383313.3412488) claims that simple matrix factorization systems such as Funk SVD outperform the newly introduced approaches if hyperparameters (such as the number of latent factors, the regularization for gradient descent, etc.) are chosen appropriately. [Another paper](https://dl.acm.org/doi/10.1145/3434185) found that 11 of 12 MF-based approaches they reproduced, which were presented at prestigious conferences, were outperformed by simpler, older approaches. In fact, they could only reproduce 12 of 26 approaches that they investigated.\n",
        "\n",
        "I would like to investigate the limits to which hyperparameter optimization could be taken for simpler approaches to MF-based recommender systems."
      ],
      "metadata": {
        "id": "9dn7Afzodsvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proposed methodology"
      ],
      "metadata": {
        "id": "6FZy9cmIiRlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, I will use Funk SVD with different numbers of latent factors and modifications of the ridge regression regularization term I use for gradient descent to investigate to what extent optimizing these hyperparameters can improve an initial approach."
      ],
      "metadata": {
        "id": "dEGXW86JiWEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment setup"
      ],
      "metadata": {
        "id": "-0csd9SFy52p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the [Surprise](https://surprise.readthedocs.io/en/stable/matrix_factorization.html) library for Python, I will investigate how changing the regularization and number of latent factors changes the error of the algorithm. I will use various data set found [here](https://github.com/caserec/Datasets-for-Recommender-Systems), to determine the accuracy of the model. I will be using RMSE as my error metric.\n",
        "\n",
        "The results will be displayed as graphs. Each data set will be associated with two graphs, one which analyzes the accuracy, recall, and precision of Funk SVD for a dataset as a function of the number of latent parameters, and one which analyzes the accuracy, recall, and precision as a function of the size of the ridge regularization. I will not be using the bias parameters specified in the Surprise library."
      ],
      "metadata": {
        "id": "S9q3scGY1fDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expected results"
      ],
      "metadata": {
        "id": "onrga9Vw392u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the number of latent factors grows, I expect the model to become more accurate until a point where it starts overfitting the data. Additionally, I expect that there is an optimal point at which the regularization function will yield the highest accuracy."
      ],
      "metadata": {
        "id": "v6pnrBr54Yku"
      }
    }
  ]
}